\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alur et~al.(2023)Alur, Brazdil, and Chawla]{alur2023}
Rajeev Alur, Pavel Brazdil, and Sanjay Chawla.
\newblock Meta-learning without memorization.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th Annual International Conference on Machine Learning (ICML)}, pages 41--48, 2009.

\bibitem[Carlsson et~al.(2023)Carlsson, Minaee, and Gidel]{carlsson2023}
Fredrik Carlsson, Shervin Minaee, and Gauthier Gidel.
\newblock The role of selection bias in the curriculum learning problem.
\newblock \emph{arXiv preprint arXiv:2301.01159}, 2023.

\bibitem[Gal(2016)]{gal2016}
Yarin Gal.
\newblock \emph{Uncertainty in deep learning}.
\newblock PhD thesis, University of Cambridge, 2016.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Wexler, Zou, and Kim]{ghorbani2020}
Amirata Ghorbani, James Wexler, James~Y Zou, and Been Kim.
\newblock Neuron shapley: Discovering the responsible neurons.
\newblock \emph{arXiv preprint arXiv:2002.09656}, 2020.

\bibitem[Hacohen and Weinshall(2019)]{hacohen2019}
Guy Hacohen and Daphna Weinshall.
\newblock Curriculum learning by transfer learning: Theory and experiments with deep networks.
\newblock In \emph{International conference on machine learning}, pages 2625--2635. PMLR, 2019.

\bibitem[Kingma and Welling(2014)]{kingma2014}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2014.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Raquel, et~al.]{kirkpatrick2017}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Krishnamurthy Milan, John Quan, Tomi Raquel, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0 (13):\penalty0 3521--3526, 2017.

\bibitem[Koh and Liang(2017)]{koh2017influence}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages 1885--1894, 2017.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010}
M~Pawan Kumar, Benjamin Packer, and Daphna Koller.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{Advances in neural information processing systems}, volume~23, pages 1189--1197, 2010.

\bibitem[Kwon and Rivas(2023)]{kwon2023}
Yongchan Kwon and Manuel~A Rivas.
\newblock Data valuation using shapley value.
\newblock \emph{arXiv preprint arXiv:2306.11554}, 2023.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{lakshminarayanan2017}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Malinin et~al.(2021)Malinin, Ahuja, McCann, and Yildirim]{malinin2021}
Andrey Malinin, Sonali Ahuja, David McCann, and Irina Yildirim.
\newblock Uncertainty estimation in one-stage object detection.
\newblock \emph{arXiv preprint arXiv:2011.02380}, 2021.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015dqn}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness, Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Owen(2013)]{owen2013}
Art~B. Owen.
\newblock \emph{Monte Carlo Theory, Methods and Examples}.
\newblock Stanford University, 2013.

\bibitem[Pruthi et~al.(2020)Pruthi, Liu, Kang, Najafi, Sundararajan, and Papernot]{pruthi2020}
Danish Pruthi, Brent Liu, Yonatan Kang, Motasem Najafi, Mukund Sundararajan, and Nicolas Papernot.
\newblock Estimating the influence of a training example.
\newblock \emph{arXiv preprint arXiv:2010.08457}, 2020.

\bibitem[Rezende and Mohamed(2015)]{rezende2015}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages 1530--1538, 2015.

\bibitem[Rubinstein and Kroese(2007)]{rubinstein2007}
Reuven~Y. Rubinstein and Dirk~P. Kroese.
\newblock \emph{Simulation and the Monte Carlo Method}.
\newblock Wiley, 2007.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and Silver]{schaul2016per}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Settles(2009)]{settles2009}
Burr Settles.
\newblock Active learning literature survey.
\newblock \emph{Computer Sciences Technical Report 1648, University of Wisconsin--Madison}, 2009.

\bibitem[Shannon(1948)]{shannon1948}
Claude~E Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{Bell system technical journal}, 27\penalty0 (3):\penalty0 379--423, 1948.

\bibitem[Shannon(1950)]{shannon1950}
Claude~E Shannon.
\newblock Programming a computer for playing chess.
\newblock \emph{The London Edinburgh Dublin Philosophical Magazine and Journal of Science}, 41\penalty0 (314):\penalty0 256--275, 1950.

\bibitem[Smith(2018)]{smith2018}
Leslie~N Smith.
\newblock A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay.
\newblock In \emph{International conference on machine learning}, pages 4693--4702. PMLR, 2018.

\bibitem[Sobczyk et~al.(2024)]{sobczyk2024nnue}
Tomasz Sobczyk et~al.
\newblock Basic training procedure (easy\_train.py).
\newblock \url{https://github.com/official-stockfish/nnue-pytorch/wiki/Basic-training-procedure-(easy_train.py)}, 2024.
\newblock Accessed: 2024-10-26.

\bibitem[{Stockfish Development Team}(2024{\natexlab{a}})]{stockfishorg2024sprt}
{Stockfish Development Team}.
\newblock Chess sprt calculator.
\newblock \url{https://tests.stockfishchess.org/sprt_calc}, 2024{\natexlab{a}}.
\newblock Sequential Probability Ratio Test parameters and expected game duration.

\bibitem[{Stockfish Development Team}(2024{\natexlab{b}})]{stockfishorg2024stats}
{Stockfish Development Team}.
\newblock Statistical methods for chess engine evaluation.
\newblock \url{https://tests.stockfishchess.org/tests/stats}, 2024{\natexlab{b}}.
\newblock SPRT, pentanomial models, and Bayesian Elo rating analysis.

\bibitem[{Stockfish Development Team}(2024{\natexlab{c}})]{stockfishorg2024testing}
{Stockfish Development Team}.
\newblock Stockfish testing infrastructure.
\newblock \url{https://tests.stockfishchess.org/tests}, 2024{\natexlab{c}}.
\newblock Distributed testing queue for Stockfish engine evaluation.

\bibitem[{Stockfish Fishtest Team}(2024)]{fishtest2024math}
{Stockfish Fishtest Team}.
\newblock Fishtest mathematics.
\newblock \url{https://github.com/official-stockfish/fishtest/wiki/Fishtest-mathematics}, 2024.
\newblock Theoretical foundations of SPRT testing and Elo rating models.

\end{thebibliography}
